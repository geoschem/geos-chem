#!/bin/bash

#SBATCH -c 48
#SBATCH -N 1
#SBATCH -t 0-12:00
#SBATCH --exclusive
#SBATCH -p huce_cascade,shared
#SBATCH --mem=15000
#SBATCH --mail-type=END

###############################################################################
### Sample GEOS-Chem Classic run script for Harvard Cannon (using SLURM).
###
### If you are running a nested-grid simulation at fine resolution, you
### will likely need to request additional memory, cores, and time.
###
### -c           : Requests this many cores
### -N           : Requests a single node
### --mem        : Requests this amount of memory in GB
### -p           : Requests these partitions where the job can run
### -t           : Requests time for the job (days-hours:minutes)
###  --exclusive : Reserves entire nodes (i.e. to prevent backfilling jobs)
###############################################################################

# Load the GNU 12.2.0 environment
source CodeDir/run/runScriptSamples/operational_examples/harvard_cannon/gcclassic.gcc12_cannon_rocky.env

# Set the proper # of threads for OpenMP
# SLURM_CPUS_PER_TASK ensures this matches the number you set with -c above
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Run GEOS_Chem and collect microarchitectural information with Intel VTune
srun -c $OMP_NUM_THREADS vtune -collect uarch-exploration -- ./gcclassic > GC.log

# Exit with return code
exit $?

