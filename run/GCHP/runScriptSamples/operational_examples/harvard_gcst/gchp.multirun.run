#!/bin/bash

#SBATCH -n 96
#SBATCH -N 2
#SBATCH -t 0-20:00
#SBATCH -p huce_cascade
#SBATCH --mem=180000
#SBATCH --mail-type=ALL
#SBATCH -o slurm-%j.out
#SBATCH -e slurm-%j.err

# See SLURM documentation for descriptions of all possible settings.
# Type 'man sbatch' at the command prompt to browse documentation.

#-------------------------------------------------------------------------
# Initial version: Lizzie Lundgren, 7/12/2018

# This run script is for use within gchp.multirun.sh which submits 
# multiple consecutive GCHP jobs to break up longer runs into smaller jobs of 
# shorter duration.
#
# NOTES:
#  1. This run script is used within gchp.multirun.sh and should not be 
#     used on its own. Use gchp.run instead if not doing multi-segmented runs.
#  2. All stdout is sent to slurm.jobid.out; each run will have a separate log.
#  3. Log file multirun.log contains info about all runs, including job ids
#     and cancellation information if a problem is encountered.
#  4. Unlike at the start of gchp.run for single segmented runs, cap_restart
#     is not deleted at the start of multi-segemented runs; the content is
#     sent to the multirun log and is used to determine if a GCHP run was
#     successful. If it does not exist at the end of the run, or if its
#     content did not change since the last run, then all jobs will be
#     cancelled and a notification about that will be sent to multirun.log.

# See SLURM documentation for descriptions of the above settings as well
# as many other settings you may use.

# Define GEOS-Chem log files, except stdout and stderr which are sent to
# slurm.jobid.out and slurm.jobid.err, as configured with the sbatch commands at
# the top of this script.
multirunlog="multirun.log"

# Define function to cancel all jobs. This is done if cap_restart does
# not exist after a run, or if its date string was not updated
cancel_all_jobs()
{
   msg="Submitted batch job"
   msgs=($(grep -hr "${msg}" ./${multirunlog}))
   for jobid in "${msgs[@]}"
   do
      if [[ ! "${msg}" = *"${jobid}"* ]]; then
         scancel ${jobid}
      fi
   done
}

# Source your environment file. This requires first setting the gchp.env
# symbolic link using script setEnvironment in the run directory. 
# Be sure gchp.env points to the same file for both compilation and run.
gchp_env=$(readlink -f gchp.env)
if [ ! -f ${gchp_env} ] 
then
   echo "ERROR: gchp.env symbolic link is not set!"
   echo "Set symbolic link to env file using setEnvironment.sh."
   echo "Exiting."
   exit 1
fi
echo "WARNING: You are using environment settings in ${gchp_env}"
source ${gchp_env}

if [ -e cap_restart ]; then
   cap_rst=$(cat cap_restart)
   echo " "
   echo "Cap_restart prior to run: ${cap_rst}"
else
   cap_rst="none"
   echo " "
   echo "No cap_restart prior to run"
fi

source runConfig.sh --silent

##---------------------------------------------------------------------------
## Below is commented out code that previously was used to update HISTORY.rc
## frequency and duration for the case of months for monthly mean diagnostics.
## It is no longer needed for monthly mean since MAPL History now allows
## monthly diagnostic configuration for time-averaged collections without
## setting number of hours in frequency and duration. The code is kept here,
## however, if needed for instantaneous collection output with frequency of
## one month, and as an example of how to configure irregular output frequency
## in general. It could be adapted to output time-averaged weekday and weekend
## files, for example
##---------------------------------------------------------------------------
#if [ -e cap_restart ]; then    
#   yr_str=${cap_rst:0:4}
#   mo_str=$((10#${cap_rst:4:2}))
#else
#   yr_str=${Start_Time:0:4}
#   mo_str=$((10#${Start_Time:4:2}))
#fi
#feb_hrs=672
#if [ "$((yr_str%4))" = "0" ]; then
#   if [ ! "$((yr_str%100))" = "0" ] || [ "$((yr_str%400))" = "0" ]; then
#      feb_hrs=696   
#   fi
#   fi
#dpm=(744 $feb_hrs 744 720 744 720 744 744 720 744 720 744)
#hrs_str=${dpm[$((mo_str-1))]}
#sed -i "s|timeAvg_freq\=.*|timeAvg_freq\=\"${hrs_str}0000\"|" ./runConfig.sh
#sed -i "s|timeAvg_dur\=.*|timeAvg_dur\=\"${hrs_str}0000\"|"   ./runConfig.sh
#sed -i "s|inst_freq\=.*|inst_freq\=\"${hrs_str}0000\"|"       ./runConfig.sh
#sed -i "s|inst_dur\=.*|inst_dur\=\"${hrs_str}0000\"|"         ./runConfig.sh
##---------------------------------------------------------------------------

echo " "
echo "Settings from runConfig.sh: "
echo " "
source runConfig.sh

# Use the last run's output restart file as the input restart file if
# cap_restart exists (meaning there was a previous run). Note that unlike
# in single runs and the first run in the multirun series, the restart file
# is set here rather than from runConfig.sh.
if [ -e cap_restart ]; then
   restart_datetime=$(echo $(cat cap_restart) | sed 's/ /_/g')
   sed -i "s|GCHPchem_INTERNAL_RESTART_FILE\:.*|GCHPchem_INTERNAL_RESTART_FILE\:     +gcchem_internal_checkpoint.restart.${restart_datetime}.nc4|" ./GCHP.rc
fi

# Only start run if no error
if [[ $? == 0 ]]; then

   NX=$( grep NX GCHP.rc | awk '{print $2}' )
   NY=$( grep NY GCHP.rc | awk '{print $2}' )
   coreCount=$(( ${NX} * ${NY} ))
   # Note that $coreCount can be less than the requested cores in #SBATCH -n
   
   # Use SLURM to distribute tasks across nodes
   planeCount=$(( ${coreCount} / ${SLURM_NNODES} ))
   if [[ $(( ${coreCount} % ${SLURM_NNODES} )) > 0 ]]; then
   	${planeCount}=$(( ${planeCount} + 1 ))
   fi
   
   # Echo info from computational cores to log file for displaying results
   echo " "
   echo "Running GCHP:"
   echo "# of cores: ${coreCount}"
   echo "# of nodes: ${SLURM_NNODES}"
   echo "cores per nodes: ${planeCount}"
   
   # Cannon-specific setting to get around connection issues at high # cores
   export OMPI_MCL_btl=openib

   # Run the simulation
   # SLURM_NTASKS is #SBATCH -n and SLURM_NNODES is #SBATCH -N above
   echo '===> Run started at' `date`
   time srun -n ${coreCount} -N ${SLURM_NNODES} -m plane=${planeCount} --mpi=pmix ./gchp
   echo '===> Run ended at' `date`
fi

# Check that cap_restart exists. If it does not, cancel all remaining jobs
if [ -f cap_restart ]; then    
   new_cap_rst=$(cat cap_restart)
   echo " "
   echo "cap_restart after run: ${new_cap_rst}" | tee -a ${multirunlog}
   if [[ "${new_cap_rst}" = "${cap_rst}" || "${new_cap_rst}" = "" ]]; then
      echo " "
      echo "Error: cap_restart did not update to different date!" >> ${multirunlog}
      echo "Cancelling all jobs." >> ${multirunlog}
      cancel_all_jobs
   else
      # Rename the restart (checkpoint) file to include datetime
      if [ -f cap_restart ]; then
         restart_datetime=$(echo $(cat cap_restart) | sed 's/ /_/g')
         mv gcchem_internal_checkpoint gcchem_internal_checkpoint.restart.${restart_datetime}.nc4
      fi
   fi
else
   echo " "
   echo "Error: cap_restart does not exist after GCHP run!" >> ${multirunlog}
   echo "Cancelling all jobs." >> ${multirunlog}
   cancel_all_jobs
fi

exit 0

